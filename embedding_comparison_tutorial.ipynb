{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Embedding Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates the calculation of Adjusted Rand Scores on a variety of directed graphs. We will compare the performance of three spectral embedding methods: Adjacency Spectral Embedding (ASE), Laplacian Spectral Embedding (LSE), and Scikit-Learn's Spectral Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.manifold import spectral_embedding\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from graspologic.embed import AdjacencySpectralEmbed, LaplacianSpectralEmbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following function generates a k-neighbors graph using latent positions derived from a block matrix. It then applies a specified spectral embedding method, followed by Gaussian Mixture Clustering to compute the Adjusted Rand Score. This process is repeated over multiple iterations, with each score being recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ari(kn_graph, n_sims, embed_method, labels_true, cov_type):\n",
    "    rows = []\n",
    "\n",
    "    for _ in range(n_sims):\n",
    "\n",
    "        #choose embedding method\n",
    "        if embed_method == \"ase\":\n",
    "            ase = AdjacencySpectralEmbed(n_components=2)\n",
    "            Xhat, Yhat = ase.fit_transform(kn_graph)\n",
    "        elif embed_method == \"lse\":\n",
    "            lse = LaplacianSpectralEmbed(n_components=2)\n",
    "            Xhat, Yhat = lse.fit_transform(kn_graph)\n",
    "        elif embed_method == \"sklearn\":\n",
    "            Xhat = spectral_embedding(kn_graph, n_components=2)\n",
    "\n",
    "        #concatenate Xhat and Yhat if using ase or lse\n",
    "        if embed_method == \"ase\" or embed_method == \"lse\":\n",
    "            Xhat = np.concatenate((Xhat, Yhat), axis=1)\n",
    "\n",
    "        #calculate the ARI score\n",
    "        gm_ic = GaussianMixture(n_components=2, covariance_type=cov_type)\n",
    "        labels_mclust = gm_ic.fit_predict(Xhat)\n",
    "        ari = adjusted_rand_score(labels_true, labels_mclust)\n",
    "\n",
    "        #save the embedding method used with the corresponding ARI value\n",
    "        result = {\n",
    "            \"test\": embed_method,\n",
    "            \"ari\": ari\n",
    "        }\n",
    "        rows.append(result)\n",
    "\n",
    "\n",
    "    results = pd.DataFrame(rows)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we present 2 cases, in which LSE performs the best in case 1, while ASE performs the best in case 2. In each case, we choose a certain number of vertices and block matrix, and run the above method using all three embedding methods. We then calculate the means of the ARI scores for each embedding method across the number of iterations to determine which embedding method gave the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simulation, we consider two block matrices, one being an affinity matrix, and the other being a core periphery matrix, as detailed in the [Two Truths paper](https://www.pnas.org/doi/10.1073/pnas.1814462116). The structure of each matrix is discussed below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "\n",
    "$$\n",
    "B_{\\text{affinity}} = \\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}, \\quad \\text{where } a, c \\gg b\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "B_{\\text{core}} = \\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}, \\quad \\text{where } a \\gg b, c \\text{ or } c \\gg b, a\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ari</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ase</th>\n",
       "      <td>0.653139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lse</th>\n",
       "      <td>0.808924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sklearn</th>\n",
       "      <td>0.311481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ari\n",
       "test             \n",
       "ase      0.653139\n",
       "lse      0.808924\n",
       "sklearn  0.311481"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#case 1: LSE does best\n",
    "n_verts_lse = 100\n",
    "n_sims = 50\n",
    "B_core = np.array([[0.011, 0.027], [0.027, 0.079]])\n",
    "labels_lse = int(n_verts_lse/2) * [0] + int(n_verts_lse/2) * [1]\n",
    "\n",
    "#generate kn graph\n",
    "#make probability matrix from block matrix\n",
    "P = np.zeros((n_verts_lse, n_verts_lse))\n",
    "P[0:int(n_verts_lse/2),0:int(n_verts_lse/2)] = B_core[0, 0]\n",
    "P[int(n_verts_lse/2):n_verts_lse, int(n_verts_lse/2):n_verts_lse] = B_core[1, 1]\n",
    "P[0:int(n_verts_lse/2), int(n_verts_lse/2):n_verts_lse] = B_core[0, 1]\n",
    "P[int(n_verts_lse/2):n_verts_lse, 0:int(n_verts_lse/2)] = B_core[1, 0]\n",
    "\n",
    "#make latent position matrix\n",
    "U, S, V = np.linalg.svd(P)\n",
    "\n",
    "#sample half the points from U\n",
    "X = U[0:int(n_verts_lse/2), 0:2] @ np.sqrt(np.diag(S[0:2]))\n",
    "\n",
    "#sample half the points from V^T\n",
    "Y = V.T[int(n_verts_lse/2):n_verts_lse, 0:2] @ np.sqrt(np.diag(S[0:2]))\n",
    "\n",
    "#concatenate the two matrices to get the full latent position matrix\n",
    "lat_mat = np.concatenate((X, Y), axis=0)\n",
    "\n",
    "#get k_neighbors graph from latent position matrix (k=sqrt(n))\n",
    "kn_graph = kneighbors_graph(lat_mat, n_neighbors=int(np.sqrt(n_verts_lse)))\n",
    "kn_graph = kn_graph.toarray()\n",
    "\n",
    "ari_aff_lse_df = calc_ari(kn_graph = kn_graph, n_sims = n_sims, embed_method = \"lse\", labels_true = labels_lse, cov_type = \"full\")\n",
    "ari_aff_ase_df = calc_ari(kn_graph = kn_graph, n_sims = n_sims, embed_method = \"ase\", labels_true = labels_lse, cov_type = \"full\")\n",
    "ari_aff_sklearn_df = calc_ari(kn_graph = kn_graph, n_sims = n_sims, embed_method = \"sklearn\", labels_true = labels_lse, cov_type = \"full\")\n",
    "ari_aff_df = pd.concat([ari_aff_lse_df, ari_aff_ase_df, ari_aff_sklearn_df])\n",
    "\n",
    "#groupby the means for each embedding method across the simulations\n",
    "ari_aff_means = ari_aff_df.groupby([\"test\"]).mean()\n",
    "ari_aff_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ari</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ase</th>\n",
       "      <td>0.656647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lse</th>\n",
       "      <td>0.524430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sklearn</th>\n",
       "      <td>0.241342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ari\n",
       "test             \n",
       "ase      0.656647\n",
       "lse      0.524430\n",
       "sklearn  0.241342"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#case 2: ASE does best\n",
    "n_verts_ase = 400\n",
    "n_sims = 50\n",
    "B_affinity = np.array([[0.050, 0.013], [0.013, 0.051]])\n",
    "labels_ase = int(n_verts_ase/2) * [0] + int(n_verts_ase/2) * [1]\n",
    "\n",
    "#generate kn graph\n",
    "#make probability matrix from block matrix\n",
    "P = np.zeros((n_verts_ase, n_verts_ase))\n",
    "P[0:int(n_verts_ase/2),0:int(n_verts_ase/2)] = B_affinity[0, 0]\n",
    "P[int(n_verts_ase/2):n_verts_ase, int(n_verts_ase/2):n_verts_ase] = B_affinity[1, 1]\n",
    "P[0:int(n_verts_ase/2), int(n_verts_ase/2):n_verts_ase] = B_affinity[0, 1]\n",
    "P[int(n_verts_ase/2):n_verts_ase, 0:int(n_verts_ase/2)] = B_affinity[1, 0]\n",
    "\n",
    "#make latent position matrix\n",
    "U, S, V = np.linalg.svd(P)\n",
    "\n",
    "#sample half the points from U\n",
    "X = U[0:int(n_verts_ase/2), 0:2] @ np.sqrt(np.diag(S[0:2]))\n",
    "\n",
    "#sample half the points from V^T\n",
    "Y = V.T[int(n_verts_ase/2):n_verts_ase, 0:2] @ np.sqrt(np.diag(S[0:2]))\n",
    "\n",
    "#concatenate the two matrices to get the full latent position matrix\n",
    "lat_mat = np.concatenate((X, Y), axis=0)\n",
    "\n",
    "#get k_neighbors graph from latent position matrix (k=sqrt(n))\n",
    "kn_graph = kneighbors_graph(lat_mat, n_neighbors=int(np.sqrt(n_verts_ase)))\n",
    "kn_graph = kn_graph.toarray()\n",
    "\n",
    "ari_core_ase_df = calc_ari(kn_graph = kn_graph, n_sims = n_sims, embed_method = \"ase\", labels_true = labels_ase, cov_type = \"full\")\n",
    "ari_core_lse_df = calc_ari(kn_graph = kn_graph, n_sims = n_sims, embed_method = \"lse\", labels_true = labels_ase, cov_type = \"full\")\n",
    "ari_core_sklearn_df = calc_ari(kn_graph = kn_graph, n_sims = n_sims, embed_method = \"sklearn\", labels_true = labels_ase, cov_type = \"full\")\n",
    "ari_core_df = pd.concat([ari_core_ase_df, ari_core_lse_df, ari_core_sklearn_df])\n",
    "ari_core_means = ari_core_df.groupby([\"test\"]).mean()\n",
    "ari_core_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
